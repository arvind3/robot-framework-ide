# AI Coach Evaluation Strategy

## Objective
Keep AI coach responses high quality and user-safe by enforcing deterministic checks before release.

## Scope
This eval suite covers:
- UI flow guidance prompts (import/export/artifacts)
- Core content explanations
- CLI command guidance
- Debug/failure handling
- Variable/structure guidance
- Beginner onboarding prompts
- Fallback prompts

## Test Case Design
- Prompt archetypes are defined in `eval/ai-coach/prompt-archetypes.json`.
- Cases are generated by cross-joining archetypes with chapter contexts from `src/bookChapters.ts`.
- Default: first 10 chapters Ã— 12 archetypes = **120 critical test cases**.

## Pass Criteria
A case passes only when:
1. All `mustInclude` terms are present.
2. All `mustNotInclude` terms are absent.

Current critical anti-pattern:
- Exposing internal context in user-facing answer (e.g., `Context:` dump).

## Execution
Run strict gate (fails non-zero if pass rate < 100):

```bash
node scripts/run-ai-coach-eval.mjs --min-pass-rate=100 --chapter-limit=10
```

Outputs:
- `eval/ai-coach/reports/latest.md`
- `eval/ai-coach/reports/latest.json`

## CI / Release Gate Recommendation
- Add this eval to CI pipeline.
- Block merge/deploy on any failure for critical severity cases.
- Require 100% pass rate for all critical cases.

## Maintenance Rules
Whenever coach behavior changes:
1. Update archetypes if product intent changed.
2. Re-run eval suite.
3. Review failed details in report.
4. Only merge when gate passes.

## Expansion Path
- Add weighted scoring by severity.
- Add stylistic checks (max length, readability grade).
- Add chapter-specific expected tokens.
- Add e2e UI checks that validate rendered coach panel text.
